# -*- coding: utf-8 -*-
"""sentiment analysis v3.1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rL_r3KOhiyK-vmuCurvqVpwp0RUVClsj

# Setup Module
"""

#@title Install Modules
# Dataset Preprocessing
!pip install -q googletrans==3.1.0a0
!pip install -q emoji
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('averaged_perceptron_tagger')

# Data Polarity
!pip install -q vaderSentiment

# Classifier
!pip install -qq transformers

#visualize
nltk.download('punkt')

#@title Import Module
# Data Preprocessing
from googletrans import Translator
import pandas as pd
import numpy as np
import re
import html
import emoji
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet as wn

# Data Polarity
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# Data Split
from sklearn.model_selection import train_test_split
from sklearn.utils import resample

# Classifier
from sklearn.metrics import classification_report
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification
from tensorflow.keras.losses import SparseCategoricalCrossentropy
import tensorflow as tf

# Visualizing
from sklearn.metrics import confusion_matrix
from nltk.tokenize import word_tokenize
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import seaborn as sns

"""# Data Preprocessing"""

#@title Emoticons

EMOTICONS = {
    u":‑\)":"Happy face or smiley",
    u":\)":"Happy face or smiley",
    u":-\]":"Happy face or smiley",
    u":\]":"Happy face or smiley",
    u":-3":"Happy face smiley",
    u":3":"Happy face smiley",
    u":->":"Happy face smiley",
    u":>":"Happy face smiley",
    u"8-\)":"Happy face smiley",
    u":o\)":"Happy face smiley",
    u":-\}":"Happy face smiley",
    u":\}":"Happy face smiley",
    u":-\)":"Happy face smiley",
    u":c\)":"Happy face smiley",
    u":\^\)":"Happy face smiley",
    u"=\]":"Happy face smiley",
    u"=\)":"Happy face smiley",
    u":‑D":"Laughing, big grin or laugh with glasses",
    u":D":"Laughing, big grin or laugh with glasses",
    u"8‑D":"Laughing, big grin or laugh with glasses",
    u"8D":"Laughing, big grin or laugh with glasses",
    u"X‑D":"Laughing, big grin or laugh with glasses",
    u"XD":"Laughing, big grin or laugh with glasses",
    u"=D":"Laughing, big grin or laugh with glasses",
    u"=3":"Laughing, big grin or laugh with glasses",
    u"B\^D":"Laughing, big grin or laugh with glasses",
    u":-\)\)":"Very happy",
    u":‑\(":"Frown, sad, andry or pouting",
    u":-\(":"Frown, sad, andry or pouting",
    u":\(":"Frown, sad, andry or pouting",
    u":‑c":"Frown, sad, andry or pouting",
    u":c":"Frown, sad, andry or pouting",
    u":‑<":"Frown, sad, andry or pouting",
    u":<":"Frown, sad, andry or pouting",
    u":‑\[":"Frown, sad, andry or pouting",
    u":\[":"Frown, sad, andry or pouting",
    u":-\|\|":"Frown, sad, andry or pouting",
    u">:\[":"Frown, sad, andry or pouting",
    u":\{":"Frown, sad, andry or pouting",
    u":@":"Frown, sad, andry or pouting",
    u">:\(":"Frown, sad, andry or pouting",
    u":'‑\(":"Crying",
    u":'\(":"Crying",
    u":'‑\)":"Tears of happiness",
    u":'\)":"Tears of happiness",
    u"D‑':":"Horror",
    u"D:<":"Disgust",
    u"D:":"Sadness",
    u"D8":"Great dismay",
    u"D;":"Great dismay",
    u"D=":"Great dismay",
    u"DX":"Great dismay",
    u":‑O":"Surprise",
    u":O":"Surprise",
    u":‑o":"Surprise",
    u":o":"Surprise",
    u":-0":"Shock",
    u"8‑0":"Yawn",
    u">:O":"Yawn",
    u":-\*":"Kiss",
    u":\*":"Kiss",
    u":X":"Kiss",
    u";‑\)":"Wink or smirk",
    u";\)":"Wink or smirk",
    u"\*-\)":"Wink or smirk",
    u"\*\)":"Wink or smirk",
    u";‑\]":"Wink or smirk",
    u";\]":"Wink or smirk",
    u";\^\)":"Wink or smirk",
    u":‑,":"Wink or smirk",
    u";D":"Wink or smirk",
    u":‑P":"Tongue sticking out, cheeky, playful or blowing a raspberry",
    u":P":"Tongue sticking out, cheeky, playful or blowing a raspberry",
    u"X‑P":"Tongue sticking out, cheeky, playful or blowing a raspberry",
    u"XP":"Tongue sticking out, cheeky, playful or blowing a raspberry",
    u":‑Þ":"Tongue sticking out, cheeky, playful or blowing a raspberry",
    u":Þ":"Tongue sticking out, cheeky, playful or blowing a raspberry",
    u":b":"Tongue sticking out, cheeky, playful or blowing a raspberry",
    u"d:":"Tongue sticking out, cheeky, playful or blowing a raspberry",
    u"=p":"Tongue sticking out, cheeky, playful or blowing a raspberry",
    u">:P":"Tongue sticking out, cheeky, playful or blowing a raspberry",
    u":‑/":"Skeptical, annoyed, undecided, uneasy or hesitant",
    u":/":"Skeptical, annoyed, undecided, uneasy or hesitant",
    u":-[.]":"Skeptical, annoyed, undecided, uneasy or hesitant",
    u">:[(\\\)]":"Skeptical, annoyed, undecided, uneasy or hesitant",
    u">:/":"Skeptical, annoyed, undecided, uneasy or hesitant",
    u":[(\\\)]":"Skeptical, annoyed, undecided, uneasy or hesitant",
    u"=/":"Skeptical, annoyed, undecided, uneasy or hesitant",
    u"=[(\\\)]":"Skeptical, annoyed, undecided, uneasy or hesitant",
    u":L":"Skeptical, annoyed, undecided, uneasy or hesitant",
    u"=L":"Skeptical, annoyed, undecided, uneasy or hesitant",
    u":S":"Skeptical, annoyed, undecided, uneasy or hesitant",
    u":‑\|":"Straight face",
    u":\|":"Straight face",
    u":$":"Embarrassed or blushing",
    u":‑x":"Sealed lips or wearing braces or tongue-tied",
    u":x":"Sealed lips or wearing braces or tongue-tied",
    u":‑#":"Sealed lips or wearing braces or tongue-tied",
    u":#":"Sealed lips or wearing braces or tongue-tied",
    u":‑&":"Sealed lips or wearing braces or tongue-tied",
    u":&":"Sealed lips or wearing braces or tongue-tied",
    u"O:‑\)":"Angel, saint or innocent",
    u"O:\)":"Angel, saint or innocent",
    u"0:‑3":"Angel, saint or innocent",
    u"0:3":"Angel, saint or innocent",
    u"0:‑\)":"Angel, saint or innocent",
    u"0:\)":"Angel, saint or innocent",
    u":‑b":"Tongue sticking out, cheeky, playful or blowing a raspberry",
    u"0;\^\)":"Angel, saint or innocent",
    u">:‑\)":"Evil or devilish",
    u">:\)":"Evil or devilish",
    u"\}:‑\)":"Evil or devilish",
    u"\}:\)":"Evil or devilish",
    u"3:‑\)":"Evil or devilish",
    u"3:\)":"Evil or devilish",
    u">;\)":"Evil or devilish",
    u"\|;‑\)":"Cool",
    u"\|‑O":"Bored",
    u":‑J":"Tongue-in-cheek",
    u"#‑\)":"Party all night",
    u"%‑\)":"Drunk or confused",
    u"%\)":"Drunk or confused",
    u":-###..":"Being sick",
    u":###..":"Being sick",
    u"<:‑\|":"Dump",
    u"\(>_<\)":"Troubled",
    u"\(>_<\)>":"Troubled",
    u"\(';'\)":"Baby",
    u"\(\^\^>``":"Nervous or Embarrassed or Troubled or Shy or Sweat drop",
    u"\(\^_\^;\)":"Nervous or Embarrassed or Troubled or Shy or Sweat drop",
    u"\(-_-;\)":"Nervous or Embarrassed or Troubled or Shy or Sweat drop",
    u"\(~_~;\) \(・\.・;\)":"Nervous or Embarrassed or Troubled or Shy or Sweat drop",
    u"\(-_-\)zzz":"Sleeping",
    u"\(\^_-\)":"Wink",
    u"\(\(\+_\+\)\)":"Confused",
    u"\(\+o\+\)":"Confused",
    u"\(o\|o\)":"Ultraman",
    u"\^_\^":"Joyful",
    u"\(\^_\^\)/":"Joyful",
    u"\(\^O\^\)／":"Joyful",
    u"\(\^o\^\)／":"Joyful",
    u"\(__\)":"Kowtow as a sign of respect, or dogeza for apology",
    u"_\(\._\.\)_":"Kowtow as a sign of respect, or dogeza for apology",
    u"<\(_ _\)>":"Kowtow as a sign of respect, or dogeza for apology",
    u"<m\(__\)m>":"Kowtow as a sign of respect, or dogeza for apology",
    u"m\(__\)m":"Kowtow as a sign of respect, or dogeza for apology",
    u"m\(_ _\)m":"Kowtow as a sign of respect, or dogeza for apology",
    u"\('_'\)":"Sad or Crying",
    u"\(/_;\)":"Sad or Crying",
    u"\(T_T\) \(;_;\)":"Sad or Crying",
    u"\(;_;":"Sad of Crying",
    u"\(;_:\)":"Sad or Crying",
    u"\(;O;\)":"Sad or Crying",
    u"\(:_;\)":"Sad or Crying",
    u"\(ToT\)":"Sad or Crying",
    u";_;":"Sad or Crying",
    u";-;":"Sad or Crying",
    u";n;":"Sad or Crying",
    u";;":"Sad or Crying",
    u"Q\.Q":"Sad or Crying",
    u"T\.T":"Sad or Crying",
    u"QQ":"Sad or Crying",
    u"Q_Q":"Sad or Crying",
    u"\(-\.-\)":"Shame",
    u"\(-_-\)":"Shame",
    u"\(一一\)":"Shame",
    u"\(；一_一\)":"Shame",
    u"\(=_=\)":"Tired",
    u"\(=\^\·\^=\)":"cat",
    u"\(=\^\·\·\^=\)":"cat",
    u"=_\^=	":"cat",
    u"\(\.\.\)":"Looking down",
    u"\(\._\.\)":"Looking down",
    u"\^m\^":"Giggling with hand covering mouth",
    u"\(\・\・?":"Confusion",
    u"\(?_?\)":"Confusion",
    u">\^_\^<":"Normal Laugh",
    u"<\^!\^>":"Normal Laugh",
    u"\^/\^":"Normal Laugh",
    u"\（\*\^_\^\*）" :"Normal Laugh",
    u"\(\^<\^\) \(\^\.\^\)":"Normal Laugh",
    u"\(^\^\)":"Normal Laugh",
    u"\(\^\.\^\)":"Normal Laugh",
    u"\(\^_\^\.\)":"Normal Laugh",
    u"\(\^_\^\)":"Normal Laugh",
    u"\(\^\^\)":"Normal Laugh",
    u"\(\^J\^\)":"Normal Laugh",
    u"\(\*\^\.\^\*\)":"Normal Laugh",
    u"\(\^—\^\）":"Normal Laugh",
    u"\(#\^\.\^#\)":"Normal Laugh",
    u"\（\^—\^\）":"Waving",
    u"\(;_;\)/~~~":"Waving",
    u"\(\^\.\^\)/~~~":"Waving",
    u"\(-_-\)/~~~ \($\·\·\)/~~~":"Waving",
    u"\(T_T\)/~~~":"Waving",
    u"\(ToT\)/~~~":"Waving",
    u"\(\*\^0\^\*\)":"Excited",
    u"\(\*_\*\)":"Amazed",
    u"\(\*_\*;":"Amazed",
    u"\(\+_\+\) \(@_@\)":"Amazed",
    u"\(\*\^\^\)v":"Laughing,Cheerful",
    u"\(\^_\^\)v":"Laughing,Cheerful",
    u"\(\(d[-_-]b\)\)":"Headphones,Listening to music",
    u'\(-"-\)':"Worried",
    u"\(ーー;\)":"Worried",
    u"\(\^0_0\^\)":"Eyeglasses",
    u"\(\＾ｖ\＾\)":"Happy",
    u"\(\＾ｕ\＾\)":"Happy",
    u"\(\^\)o\(\^\)":"Happy",
    u"\(\^O\^\)":"Happy",
    u"\(\^o\^\)":"Happy",
    u"\)\^o\^\(":"Happy",
    u":O o_O":"Surprised",
    u"o_0":"Surprised",
    u"o\.O":"Surpised",
    u"\(o\.o\)":"Surprised",
    u"oO":"Surprised",
    u"\(\*￣m￣\)":"Dissatisfied",
    u"\(‘A`\)":"Snubbed or Deflated",
    u"<3":"Love"}

#@title Functions
def detect_lang(text):
  translator = Translator()
  res = translator.translate(text)
  return res.src

def html_trans(text):
  html_pattern = re.compile('<.*?>')
  text1 = html_pattern.sub(r'', text)
  text = html.unescape(text1) #translate html
  return text

def remove_punct(text):
  for emot in EMOTICONS:
    text = re.sub(u'('+emot+')', " ".join(EMOTICONS[emot].replace(",","! ").split()), text)
  text = emoji.demojize(text)
  text = re.sub(r'(?<=[.,])(?=[^\s])', r' ', text) #translate emote
  text = re.sub("[^a-zA-Z'_]+", ' ', text)
  text = text.replace("_", " ") #remove punctuation
  return text

sw = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
wordnet_map = {"N":wn.NOUN, "V":wn.VERB, "J":wn.ADJ, "R":wn.ADV}

def lemmatize(text):
  text = " ".join([word for word in str(text).split() if word not in sw]) #remove stopwords
  pos_tagged_text = nltk.pos_tag(text.split())
  text = " ".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wn.NOUN)) for word, pos in pos_tagged_text]) #lemmatize word
  return text

print(sw)

ds = pd.read_csv('/content/drive/MyDrive/Skripsi/YT Comment v1.1 - Shogun no reply.csv')
ds = ds.dropna()
ds.info()

ds

ds['html'] = ds.Comment.apply(lambda text: html_trans(text))
ds['punct'] = ds.html.apply(lambda text: remove_punct(text))
ds['punct'] = ds.punct.str.lower()
ds['lem'] = ds.punct.apply(lambda text: lemmatize(text))
ds.head()

ds['lang'] = ds.Comment.apply(lambda text: detect_lang(text))
ds = ds[ds.lang == 'en']
ds.shape

ds.lem.replace('', np.nan, inplace=True)
ds.dropna(subset=['lem'], inplace = True)
ds.head()

ds.to_csv('/content/drive/MyDrive/Skripsi/dataset/dataset_shogun_clean.csv')

"""# Data Polarity"""

#@title Function
def sentiment_score(text):
    vader = SentimentIntensityAnalyzer()
    sentiment_dict = vader.polarity_scores(text)
    val = sentiment_dict['compound']
    return val

def sentiment_label(num):
    if num > 0:
      val = "positif"
    elif num < 0:
      val = "negatif"
    else:
      val = "undefined"
    return val

df_vader = pd.read_csv('/content/drive/MyDrive/Skripsi/dataset/dataset_shogun_clean.csv', lineterminator = '\n')
df_vader

df_vader['auto_polar_val'] = df_vader.html.apply(lambda text: sentiment_score(text))
df_vader['auto_polar'] = df_vader.auto_polar_val.apply(lambda num: sentiment_label(num))
df_vader.head()

df_vader['sentiment'] = df_vader.auto_polar
df_vader.sentiment[df_vader.sentiment == 'positif'] = 1
df_vader.sentiment[df_vader.sentiment == 'negatif'] = 0
df_vader.head()

df_vader = df_vader[df_vader.auto_polar.str.contains("undefined")==False]
df_vader.auto_polar.value_counts()

df_vader.to_csv('/content/drive/MyDrive/Skripsi/dataset/dataset_shogun_polar.csv')

"""# Data Split"""

df = pd.read_csv('/content/drive/MyDrive/Skripsi/dataset/dataset_shogun_polar.csv', lineterminator = '\n')
df.auto_polar.value_counts()

#@title Downsampling
neg_txt = df[df.auto_polar == 'negatif'].sample(n = 1000, random_state = 42)
pos_txt = df[df.auto_polar == 'positif']

pos_down = resample(pos_txt,
             replace=True,
             n_samples=len(neg_txt),
             random_state=42)

ds = pd.concat([neg_txt, pos_down])
ds.auto_polar.value_counts()

#@title Splitting
RANDOM_SEED = 42

df_train, df_test = train_test_split(
  ds,
  test_size=0.2,
  random_state=RANDOM_SEED
)

df_train['train_type'] = 'train'
df_test['train_type'] = 'test'
df = pd.concat([df_train, df_test])

print(df.train_type.value_counts(), df_train.auto_polar.value_counts())

df.to_csv('/content/drive/MyDrive/Skripsi/dataset/dataset_shogun_split.csv')

"""# Train Classifier

## Load Data
"""

df = pd.read_csv('/content/drive/MyDrive/Skripsi/dataset/dataset_shogun_split.csv', lineterminator = '\n')
df_train = df[df.train_type == 'train']
df_test = df[df.train_type == 'test']
print(df_train.shape, df_test.shape)

df_test.auto_polar.value_counts()

df_train.auto_polar.value_counts()

df.head()

"""##TF IDF"""

tfidf = TfidfVectorizer()
train_tfidf = tfidf.fit_transform(df_train.lem.astype('U'))
test_tfidf = tfidf.transform(df_test.lem.astype('U'))
print(train_tfidf)

x1 = df_train[['Comment', 'lem']]
x2 = pd.DataFrame(train_tfidf.toarray(), columns=tfidf.get_feature_names_out())
x = pd.concat([x1, x2], axis=1, join='inner')
x.head()

x2

"""## Naive Bayes"""

mnb = MultinomialNB()
mnb.fit(train_tfidf, df_train.auto_polar)

nb_pred = mnb.predict(test_tfidf)
print(nb_pred)

x1 = pd.DataFrame(mnb.predict_proba(test_tfidf))
x1['value'] = nb_pred
x2 = df_test[['Comment', 'lem']].reset_index().drop(['index'], axis=1)
x = pd.concat([x2, x1], axis=1, join='inner')
x.head()

print(classification_report(nb_pred, df_test.auto_polar))

"""## SVM"""

svm = SVC(kernel='linear')
svm.fit(train_tfidf, df_train.auto_polar)

svm_pred = svm.predict(test_tfidf)
print(svm_pred)

x = df_test[['Comment', 'lem']].reset_index().drop(['index'], axis=1)
x['SVM'] = svm_pred
x.head()

print(classification_report(df_test.auto_polar, svm_pred))

"""## Bert"""

tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')
model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')

def tokenize_text(data):
  encoded = tokenizer(data, padding=True, truncation=True, return_tensors='np')
  return encoded.data

y_train = df_train.sentiment
x_train = df_train.lem.tolist()
train_data = tokenize_text(x_train)

y_test = df_test.sentiment
x_test = df_test.lem.tolist()
test_data = tokenize_text(x_test)

print(train_data)

train_data['input_ids'].shape,y_train.shape

optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)
model.compile(optimizer=optimizer, metrics=['accuracy'])
model.fit(
    train_data,
    np.array(y_train), 
    validation_data=(
        test_data,
        np.array(y_test),
    ),
    batch_size=16, epochs=3
)

model.save_pretrained('bert_1')

model = TFDistilBertForSequenceClassification.from_pretrained('bert_1')

preds=model.predict(test_data)
bert_tf=np.argmax(preds['logits'],axis=1)
print(bert_tf)

bert_pred = np.where(bert_tf == 1, 'positif', 'negatif')
x1 = pd.DataFrame(bert_tf)
x1['bert'] = bert_pred
x2 = df_test[['Comment', 'lem']].reset_index().drop(['index'], axis=1)
x = pd.concat([x2, x1], axis=1, join='inner')
x.head()

print(classification_report(bert_pred,df_test.auto_polar))

"""## Vote"""

def vote(x, y, z):
  vote = pd.DataFrame()
  vote['x'] = x
  vote['y'] = y
  vote['z'] = z
  vote['vote'] = vote.mode(axis = 1)[0]
  vote_pred = vote.vote.to_numpy()
  return vote_pred

vote_pred = vote(bert_pred, nb_pred, svm_pred)
print(vote_pred)

print(classification_report(df_test.auto_polar, vote_pred))

df_test['nb'] = nb_pred
df_test['svm'] = svm_pred
df_test['bert'] = bert_pred
df_test['vote'] = vote_pred
df_test.head()

"""## Saving Data"""

df_test.to_csv('/content/drive/MyDrive/Skripsi/dataset/dataset_shogun_train_result.csv')

"""# Classifying Text"""

df = pd.read_csv('/content/drive/MyDrive/Skripsi/dataset/dataset_shogun_clean.csv', lineterminator = '\n')
df.head()

#@title TFIDF
df_tfidf = tfidf.transform(df.lem.astype('U'))

#@title NB Pred
nb_pred = mnb.predict(df_tfidf)
df['nb'] = nb_pred
df.nb.value_counts()

#@title SVM Pred
svm_pred = svm.predict(df_tfidf)
df['svm'] = svm_pred
df.svm.value_counts()

a = df.lem.tolist()
token_data = tokenize_text(a)

preds=model.predict(token_data)
bert_tf=np.argmax(preds['logits'],axis=1)

bert_pred = np.where(bert_tf == 1, 'positif', 'negatif')
df['bert'] = bert_pred
df.bert.value_counts()

#@title Vote Pred
vote_pred = vote(bert_pred, nb_pred, svm_pred)
df['vote'] = vote_pred
df.vote.value_counts()

df.head()

df.to_csv('/content/drive/MyDrive/Skripsi/dataset/dataset_shogun_classified.csv')

"""# Model Evaluation"""

df_test = pd.read_csv('/content/drive/MyDrive/Skripsi/dataset/dataset_shogun_train_result.csv', lineterminator = '\n')
nb_pred = df_test.nb
svm_pred = df_test.svm
bert_pred = df_test.bert
vote_pred = df_test.vote
df_test.head(3)

#@title Confusion Matrix
cm = confusion_matrix

nbcm = cm(df_test.auto_polar, nb_pred)
svmcm = cm(df_test.auto_polar, svm_pred)
bertcm = cm(df_test.auto_polar, bert_pred)
votecm = cm(df_test.auto_polar, vote_pred)

fig, ax = plt.subplots(2, 2, figsize = (10, 10))
ax[0,0].set_title('Naive Bayes')
sns.heatmap(nbcm/np.sum(nbcm), annot = True, fmt = '.1%', cmap = 'Blues', ax = ax[0, 0])

ax[0,1].set_title('SVM')
sns.heatmap(svmcm/np.sum(svmcm), annot = True, fmt = '.1%', cmap = 'BuGn', ax = ax[0, 1])

ax[1,0].set_title('BERT')
sns.heatmap(bertcm/np.sum(bertcm), annot = True, fmt = '.1%', cmap = 'BuPu', ax = ax[1, 0])

ax[1,1].set_title('Vote')
sns.heatmap(votecm/np.sum(votecm), annot = True, fmt = '.1%', cmap = 'OrRd', ax = ax[1, 1])

#@title Report
print('naive bayes report')
print(classification_report(df_test.auto_polar, nb_pred))

print('svm report')
print(classification_report(df_test.auto_polar, svm_pred))

print('bert report')
print(classification_report(df_test.auto_polar, bert_pred))

print('vote report')
print(classification_report(df_test.auto_polar, vote_pred))

#@title extract report
def vis_rep(x, y, text):
  title = text
  def cls_rep_pd (x, y):
    z = classification_report(x, y, output_dict=True)
    a = pd.DataFrame(z).transpose()
    return a

  c = cls_rep_pd(x, y)
  acc = c.iloc[3][2]
  acc = round(acc, 2)

  prec = c.iloc[4][0]
  prec = round(prec, 2)

  rec = c.iloc[4][1]
  rec = round(rec, 2)

  f1 = c.iloc[4][2]
  f1 = round(f1, 2)

  def plot_rep(a, title):
    b = a.drop(['support'], axis = 1).iloc[:2]
    plt.title(title)
    plt.plot(b)
    plt.legend(['precission', 'recall', 'f1'])
    plt.annotate
    return plt.show()

  return plot_rep(c, title), acc, prec, rec, f1

nb_plt, nb_acc, nb_prec, nb_rec, nb_f1 = vis_rep(df_test.auto_polar, nb_pred, 'Naive Bayes')

svm_plt, svm_acc, svm_prec, svm_rec, svm_f1 = vis_rep(df_test.auto_polar, svm_pred, 'SVM')

bert_plt, bert_acc, bert_prec, bert_rec, bert_f1 = vis_rep(df_test.auto_polar, bert_pred, 'BERT')

vote_plt, vote_acc, vote_prec, vote_rec, vote_f1 = vis_rep(df_test.auto_polar, vote_pred, 'Vote')

bert_acc

#@title Report Visualize
x = ['nb', 'svm', 'bert', 'vote']
a = [nb_acc, svm_acc, bert_acc, vote_acc]
b = [nb_prec, svm_prec, bert_prec, vote_prec]
c = [nb_rec, svm_rec, bert_prec, vote_prec]
d = [nb_f1, svm_f1, bert_f1, vote_f1]
y = pd.DataFrame({'label':x,
                 'acc':a,
                 'prec':b,
                 'rec':c,
                 'f1':d})
y = pd.melt(y, id_vars='label', value_name='percent')

sns.catplot(x='variable', y='percent', hue='label', kind='bar', data=y, palette='mako').set(title='Report').set(ylim=(0.775, 0.9))

"""# Text Visualization"""

df = pd.read_csv('/content/drive/MyDrive/Skripsi/dataset/dataset_shogun_classified.csv', lineterminator = '\n')
df.head(3)

df.info()

#@title Sentiment Ratio
x = list(df.vote.value_counts())
y = ['positif', 'negatif']
plt.pie(x, labels = y, autopct = '%1.1f%%', startangle = 90, colors = sns.color_palette('bwr'))
plt.title('Sentiment')

"""## Word Frequencies"""

#@title All Data
stopwords = set(STOPWORDS) 

txt = ''
for text in df.punct:
    tokens = str(text).split()
    tokens = [i.lower() for i in tokens]
    txt += ' '.join(tokens) + ' '

txt_tokens = word_tokenize(txt)
filtered = [w for w in txt_tokens if not w.lower() in stopwords]
txt = (" ").join(filtered)

def word_count(str):
    counts = dict()
    words = str.split()

    for word in words:
        if word in counts:
            counts[word] += 1
        else:
            counts[word] = 1

    return counts

head_num = 20

freq = word_count(txt)
x = freq.keys()
y = freq.values()
ds = pd.DataFrame()
ds['word'] = x
ds['freq'] = y
ds = ds[ds.word.str.len() > 2]
top_freq = ds.sort_values('freq', ascending = False).head(head_num)
x = top_freq.word
y = top_freq.freq

plt.barh(x, y, color = sns.color_palette('Spectral_r', len(range(head_num))))
plt.title("'Word Frequencies'")

stopwords = set(STOPWORDS) 

x_pos = df.punct[df.vote == 'positif']

txt_pos = ''
for text in x_pos:
    tokens = str(text).split()
    tokens = [i.lower() for i in tokens]
    txt_pos += ' '.join(tokens) + ' '

txt_tokens = word_tokenize(txt_pos)
filtered = [w for w in txt_tokens if not w.lower() in stopwords]
txt = (" ").join(filtered)

def word_count(str):
    counts = dict()
    words = str.split()

    for word in words:
        if word in counts:
            counts[word] += 1
        else:
            counts[word] = 1

    return counts

head_num = 20

freq = word_count(txt)
x = freq.keys()
y = freq.values()
ds_pos = pd.DataFrame()
ds_pos['word'] = x
ds_pos['freq'] = y

x_neg = df.punct[df.vote == 'negatif']

txt_neg = ''
for text in x_neg:
    tokens = str(text).split()
    tokens = [i.lower() for i in tokens]
    txt_neg += ' '.join(tokens) + ' '

txt_tokens = word_tokenize(txt_neg)
filtered = [w for w in txt_tokens if not w.lower() in stopwords]
txt = (" ").join(filtered)

freq = word_count(txt)
x = freq.keys()
y = freq.values()
ds_neg = pd.DataFrame()
ds_neg['word'] = x
ds_neg['freq'] = y

ds_neg['sen'] = "neg"
ds_pos['sen'] = "pos"
dsx = pd.concat([ds_neg, ds_pos])
dsx = dsx.drop_duplicates(subset="word", keep=False)
ds_pos = dsx[dsx['sen']=="pos"]
ds_neg = dsx[dsx['sen']=="neg"]

"""##WordCloud"""

#@title positif
ds_pos = ds_pos[ds_pos.word.str.len() > 2]
top_freq = ds_pos.sort_values('freq', ascending = False).head(head_num)
x = top_freq.word
y = top_freq.freq

pos_plt = plt.barh(x, y, color = sns.color_palette('Blues_r', len(range(head_num))))
plt.title("'Positif'")

#@title negatif
ds_neg = ds_neg[ds_neg.word.str.len() > 2]
top_freq = ds_neg.sort_values('freq', ascending = False).head(head_num)
x = top_freq.word
y = top_freq.freq

neg_plt = plt.barh(x, y, color = sns.color_palette('Reds_r', len(range(head_num))))
plt.title("'Negatif'")

#@title wc pos
data = ds_pos.set_index('word').to_dict()['freq']
poscol = 250.0
def pos_col(word=None, font_size=None, position=None, orientation=None, font_path=None, random_state=None):
    h = int(360.0 * poscol / 360)
    s = int(100.0 * 100.0 / 100.0)
    l = int(100.0 * float(random_state.randint(15, 85)) / 100.0)
    return "hsl({}, {}%, {}%)".format(h, s, l)
wc = WordCloud(width=800, height=400, max_words=200, background_color="rgba(255, 255, 255, 0)", color_func=pos_col,).generate_from_frequencies(data)
plt.figure(figsize=(10, 10))
plt.imshow(wc, interpolation='bilinear')
plt.axis('off')
plt.show()

#@title wc neg
negcol = 360.0
def neg_col(word=None, font_size=None, position=None, orientation=None, font_path=None, random_state=None):
    h = int(360.0 * negcol / 360)
    s = int(100.0 * 100.0 / 100.0)
    l = int(100.0 * float(random_state.randint(15, 85)) / 100.0)
    return "hsl({}, {}%, {}%)".format(h, s, l)
data = ds_neg.set_index('word').to_dict()['freq']
wc = WordCloud(width=800, height=400, max_words=200, background_color="rgba(255, 255, 255, 0)", color_func=neg_col,).generate_from_frequencies(data)
plt.figure(figsize=(10, 10))
plt.imshow(wc, interpolation='bilinear')
plt.axis('off')
plt.show()

"""## Comment based on words"""

x = df[df['vote'] == 'positif']
y = df[df['vote'] == 'negatif']

x = x.punct[x['punct'].str.contains('worthy|unknown|supremacyi|clapping|sparkling|woof|sustainer|supremacy|glory|effort')]
y = y.punct[y['punct'].str.contains('bla|dislikes|alone|construction|curving|joke|worried|kills|cies|flower')]

dsp = pd.DataFrame()
dsn = pd.DataFrame()
dsp['word'] = x
dsp['sen'] = "pos"
dsn['word'] = y
dsn['sen'] = "neg"
dfs = pd.concat([dsp, dsn])
dsp.shape, dsn.shape

dfs

dfs.word.dropna()
dfs.to_csv('/content/drive/MyDrive/Skripsi/dataset/dataset_shogun_common_words.csv')